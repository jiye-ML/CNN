* 于是乎我们有了以下的经验总结：
    1. sigmoid在压缩数据幅度方面有优势，对于深度网络，使用sigmoid可以保证数据幅度不会有问题，这样数据幅度稳住了就不会出现太大的失误。
    2. sigmoid存在梯度消失的问题，在反向传播上有劣势，所以在优化的过程中存在不足
    3. relu不会对数据做幅度压缩，所以如果数据的幅度不断扩张，那么模型的层数越深，幅度的扩张也会越厉害，最终会影响模型的表现。
    4. 但是relu在反向传导方面可以很好地将“原汁原味”的梯度传到后面，这样在学习的过程中可以更好地发挥出来。
    