'''https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/'''import numpy as np# 输入层X = np.array([[1, 0, 1, 0], [1, 0, 1, 1], [0, 1, 0, 1]])# 输出层y = np.array([[1], [1], [0]])# 激活函数def sigmoid(x):    return 1 / (1 + np.exp(-x))# 激活函数的导数def derivatives_sigmoid(x):    return x * (1 - x)# 变量初始化epoch = 5000learning_rate = 0.1input_neurons = X.shape[1]hidden_neurons = 3output_neurons = 1# 权重和偏置初始化wh = np.random.uniform(size=(input_neurons, hidden_neurons))bh = np.random.uniform(size=(1, hidden_neurons))w_out = np.random.uniform(size=(hidden_neurons, output_neurons))b_out = np.random.uniform(size=(1, output_neurons))for i in range(epoch):    # forward    hidden = sigmoid(np.dot(X, wh) + bh)    output = sigmoid(np.dot(hidden, w_out) + b_out)    # backward    error = y - output    slope_output = derivatives_sigmoid(output)    slopt_hidden = derivatives_sigmoid(hidden)    d_output = error * slope_output    error_hidden = d_output.dot(w_out.T)    d_hidden = error_hidden * slopt_hidden    w_out += hidden.T.dot(d_output) * learning_rate    b_out += np.sum(d_output, axis=0, keepdims=True) * learning_rate    wh += X.T.dot(d_hidden) * learning_rate    bh += np.sum(d_hidden, axis=0, keepdims=True) * learning_rateprint(output)