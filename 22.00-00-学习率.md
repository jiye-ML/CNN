* [Tensorflow中learning rate decay的奇技淫巧](https://zhuanlan.zhihu.com/p/32923584)
    * exponential_decay \
    ![learning_rate_exponential_decay](readme/learning_rate_exponential_decay.jpg)
    * piecewise_constant:分段常数下降法类似于exponential_decay中的阶梯式下降法，不过各阶段的值是自己设定的。\
    ![learning_rate_piecewise_constant](readme/learning_rate_piecewise_constant.jpg)
    * polynomial_decay: polynomial_decay是以多项式的方式衰减学习率的。
    ```
    global_step = min(global_step, decay_steps)
    decayed_learning_rate = (learning_rate - end_learning_rate)*(1-global_step / decay_steps)^(power)+ end_learning_rate
    ```
    ![learning_rate_polynomial_decay](readme/learning_rate_polynomial_decay.jpg)
    * natural_exp_decay: natural_exp_decay和exponential_decay形式差不多，只不过自然指数下降的底数是 1/e 型。\
    ![learning_rate_natural_exp_decay](readme/learning_rate_natural_exp_decay.jpg)
    * inverse_time_decay:
    ```
    decayed_learning_rate = learning_rate / (1 + decay_rate * global_step / decay_step)
    ```
    ![learning_rate_inverse_time_decay](readme/learning_rate_inverse_time_decay.jpg)
    * cosine_decay: alpha的作用可以看作是baseline，保证lr不会低于某个值。
    ```
    global_step = min(global_step, decay_steps)
    cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
    decayed = (1 - alpha) * cosine_decay + alpha
    decayed_learning_rate = learning_rate * decayed
    ```
    ![cosine_decay](readme/learning_rate_cosine_decay.jpg)
    * cosine_decay_restarts:cosine_decay的cycle版本。first_decay_steps是指第一次完全下降的step数，
    t_mul是指每一次循环的步数都将乘以t_mul倍，m_mul指每一次循环重新开始时的初始lr是上一次循环初始值的m_mul倍。
    ![cosine_decay_restarts](readme/learning_rate_cosine_decay_restart.jpg)
    * linear_cosine_decay: 主要应用领域是增强学习领域，本人未尝试过。可以看出，该方法也是基于余弦函数的衰减策略。
    ![linear_cosine_decay](readme/learning_rate_linear_cosine_decay.jpg)
    * noisy_linear_cosine_decay: 
    * auto_learning_rate_decay:

    
    


## 论文

### 《SGDR: Stochastic Gradient Descent with Warm Restarts》
* [paper](paper/2017-SGDR-%20Stochastic%20Gradient%20Descent%20with%20Warm%20Restarts.pdf)
* cosine_decay: alpha的作用可以看作是baseline，保证lr不会低于某个值。
```
global_step = min(global_step, decay_steps)
cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
decayed = (1 - alpha) * cosine_decay + alpha
decayed_learning_rate = learning_rate * decayed
```



### Neural Optimizer Search with Reinforcement Learning
* [paper](paper/2017-Neural%20Optimizer%20Search%20with%20Reinforcement%20Learning.pdf)
* linear_cosine_decay


### Don't Decay the Learning Rate, Increase the Batch Size
* [paper](paper/2018-Don't%20Decay%20the%20Learning%20Rate,%20Increase%20the%20Batch%20Size.pdf)
* [学界 | 抛弃Learning Rate Decay吧！](https://mp.weixin.qq.com/s/wUihQ7uYH4rUQ4gnFnLdDw)
    * 摘要：
        1. 实际上作者在衰减学习率的时候同时也降低了SGD中随机波动的值；衰减学习率类似于模拟退火；
        2. 不同于衰减学习率，提出了在增加 Batch Size 的同时保持学习率的策略，既可以保证不掉点，还可以减少参数更新的次数；
        3. 增加学习率又增大 Batch Size，如此可以基本保持test中不掉点又进一步减少参数更新次数；
        4. 对比了自己的模型和另一篇著名论文(Accurate, large minibatch SGD: Training imagenet in 1 hour)中的模型，
        Batch Size：65536 - 8192；正确率：77% - 76%；参数更新次数：2500 - 14000；