# 梯度计算

在上几个视频中，我们了解到为了最小化误差函数，我们需要获得一些导数。我们开始计算误差函数的导数吧。首先要注意的是 s 型函数具有很完美的导数。即

σ′(x)=σ(x)(1−σ(x))\sigma&#x27;(x) = \sigma(x) (1-\sigma(x))σ′(x)=σ(x)(1−σ(x))

原因是，我们可以使用商式计算它：

### 

​    ![img](file:///E:/course/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20v2.0.0/Part%2008-Module%2001-Lesson%2001_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/img/codecogseqn-49.gif)                

### 

现在，如果有
 mmm
 个样本点，标为
 x(1),x(2),…,x(m),x^{(1)}, x^{(2)}, \ldots, x^{(m)},x(1),x(2),…,x(m),
 误差公式是：

E=−1m∑i=1m(y(i)ln⁡(y(i)^)+(1−y(i))ln⁡(1−y(i)^))E = -\frac{1}{m} \sum_{i=1}^m \left( y^{(i)} \ln(\hat{y^{(i)}}) + (1-y^{(i)}) \ln (1-\hat{y^{(i)}}) \right)E=−m1∑i=1m(y(i)ln(y(i)^)+(1−y(i))ln(1−y(i)^))

预测是
 y(i)^=σ(Wx(i)+b).\hat{y^{(i)}} = \sigma(Wx^{(i)} + b).y(i)^​=σ(Wx(i)+b).

我们的目标是计算
 E,E,E,
 在单个样本点 x
 时的梯度（偏导数），其中 x 包含 n 个特征，即x=(x1,…,xn),x = (x_1, \ldots, x_n),x=(x1​,…,xn​),。

∇E=(∂∂w1E,⋯&ThinSpace;,∂∂wnE,∂∂bE)\nabla E =\left(\frac{\partial}{\partial w_1}E, \cdots, \frac{\partial}{\partial w_n}E, \frac{\partial}{\partial b}E \right)∇E=(∂w1∂E,⋯,∂wn∂E,∂b∂E)

为此，首先我们要计算
 ∂∂wjy^.\frac{\partial}{\partial w_j} \hat{y}.∂wj​∂​y^​.

y^=σ(Wx+b),\hat{y} = \sigma(Wx+b),y^=σ(Wx+b),
 因此：

### 

​    ![img](file:///E:/course/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20v2.0.0/Part%2008-Module%2001-Lesson%2001_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/img/codecogseqn-43.gif)                

### 

最后一个等式是因为和中的唯一非常量项相对于
 wjw_jwj​
 正好是
 wjxj,w_j x_j,wj​xj​,
 明显具有导数
 xj.x_j.xj​.

现在可以计算
 ∂∂wjE\frac {\partial} {\partial w_j} E∂wj​∂​E

### 

​    ![img](file:///E:/course/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20v2.0.0/Part%2008-Module%2001-Lesson%2001_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/img/codecogseqn-45.gif)                

### 

类似的计算将得出：（备注：下图公式缺少一个负号，且其为 m 个样本点时的公式）

【针对单个样本点时，E 对 b 求偏导的公式为：∂∂bE=−(y−y^)\frac {\partial} {\partial b} E=-(y -\hat{y})∂b∂E=−(y−y^)】

### 

​    ![img](file:///E:/course/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20v2.0.0/Part%2008-Module%2001-Lesson%2001_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/img/codecogseqn-50.gif)                

### 

这个实际上告诉了我们很重要的规则。对于具有坐标
 (x1,…,xn),(x_1, \ldots, x_n),(x1​,…,xn​),
 的点，标签
 y,y,y,
 预测
 y^,\hat{y},y^​,
 该点的误差函数梯度是
 (−(y−y^)x1,⋯&ThinSpace;,−(y−y^)xn,−(y−y^)).\left(-(y - \hat{y})x_1, \cdots, -(y - \hat{y})x_n, -(y - \hat{y}) \right).(−(y−y^​)x1​,⋯,−(y−y^​)xn​,−(y−y^​)).

总之

∇E(W,b)=−(y−y^)(x1,…,xn,1).\nabla E(W,b) = -(y - \hat{y}) (x_1, \ldots, x_n, 1).∇E(W,b)=−(y−y^)(x1,…,xn,1).

如果思考下，会发现很神奇。梯度实际上是标量乘以点的坐标！什么是标量？也就是标签和预测直接的差别。这意味着，如果标签与预测接近（表示点分类正确），该梯度将很小，如果标签与预测差别很大（表示点分类错误），那么此梯度将很大。请记下：小的梯度表示我们将稍微修改下坐标，大的梯度表示我们将大幅度修改坐标。

如果觉得这听起来像感知器算法，其实并非偶然性！稍后我们将详细了解。