* 如果网络的输出为c：
```Out[23]: 
tensor([-0.1227,  0.9241, -0.9106, -1.2847, -5.2645,  0.5783, -0.2082, -5.6547],
       device='cuda:0', grad_fn=<SelectBackward>)
```
* 经过一次softmax之后结果为a
```
Out[24]: 
tensor([0.1323, 0.3768, 0.0602, 0.0414, 0.0008, 0.2666, 0.1214, 0.0005],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
```

* 经过两次softmax之后的结果为b
```
Out[25]: 
tensor([0.1249, 0.1595, 0.1162, 0.1140, 0.1095, 0.1429, 0.1235, 0.1095],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
```


* 从上面c可以看出了每个输出都趋同， 如果softmax的输入的每个值是相同的，
 [1/10, 1/10, 1/10, 1/10]，则根据softmax公式输出也会相同[1/10, 1/10, 1/10, 1/10]， 相当于
网络没有学到任何的差异

* 是不是 softmax 能够感知输出的变化范围， 如果是变化很大的话， 通过归一化到 01之间的值，
打的值就接近1， 小的值接近0， 但是如果经过一次softmax， 数值之间的差异就不是很大了， 
然后softmax就感觉每个值都是差不多的， 衡量起来