* 如果网络的输出为c：
```Out[23]: 
tensor([-0.1227,  0.9241, -0.9106, -1.2847, -5.2645,  0.5783, -0.2082, -5.6547],
       device='cuda:0', grad_fn=<SelectBackward>)
```
* 经过一次softmax之后结果为a
```
Out[24]: 
tensor([0.1323, 0.3768, 0.0602, 0.0414, 0.0008, 0.2666, 0.1214, 0.0005],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
```

* 经过两次softmax之后的结果为b
```
Out[25]: 
tensor([0.1249, 0.1595, 0.1162, 0.1140, 0.1095, 0.1429, 0.1235, 0.1095],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
```


* 从上面c可以看出了每个输出都趋同， 如果softmax的输入的每个值是相同的，
 [1/10, 1/10, 1/10, 1/10]，则根据softmax公式输出也会相同[1/10, 1/10, 1/10, 1/10]， 相当于
网络没有学到任何的差异