### 《Rectified nonlinearities improve neural network acstic models》

* [paper](paper/2013-Rectified%20nonlinearities%20improve%20neural%20network%20acstic%20models.pdf)
* why
    * 它的0值以外的偏导数是1。因此梯度消失不存在。 另外， ReLU在除了0以外的地方饱和，这潜在帮助了当隐层单元激活作为输入特征，分类的。
    * 如果我们相信稀疏激活对于一个隐藏单元的是重要的对于输入刺激的不变性，那么ReLU有很明显的优势。
   