# hyperparameter tuning

* idea
* code
* experiment


## train/ dev / test

* 验证集的目的是为了帮你快速评估算法，以便你可以判断算法A或B谁更好。
如果被错误标注的验证集的一小部分妨碍你做出这些判断的能力，那么花时间去修正错误标注的验证集标签是值得的。

* overfit：
    * 你的数据要同分布， 寻找更多的训练集
    * 但是如果训练集和测试集来自不同的分布，那么你的选择是不清晰的。几方面可能会出错：
        1. 在开发集上过拟合。
        2. 测试集比开发集更难。所以你的算法可能做的和预期一样好，因此没有进一步的重大改进的可能了。
        3. 测试集不一定更难，但只是和开发集不同。所以在开发集上表现很好但并不能在测试集上表现一样。
        这种情况下，之前很多提高开发集性能的努力可能都白费了
* 使用单一数字的指标来评估你的算法，比如准确率。
    * 如果使用查全率和查准率那么可以使用F1 score这个单一指标度量，这是一种基于其平均值改善的方法，比简单地取平均值效果要好
    ![](2.提升深度升级网络-超参数调节,%20正则化，优化/F1-score.jpg)
    * 当你面在大量的分类器中进行选择时，使用单一数字的评估指标可以加快你做出决策的能力。
    所有这些都给出了明确的表现排名，从而给出一个清晰的前进方向。
* Optimizing and satisficing metrics
    * satisficing metrics: 你能接受的时间阈值是多少
    * Optimizing： 在这个阈值下准确率越高越好
* 机器学习是一个高度迭代的过程：在发现你满意的方法之前你可能需要尝试很多的idea。

### bias 和 variance

* 理想误差: 人识别的误差，
* bias： 训练集上的训练误差
* variance： 测试集上的训练误差


### 机器学习基本recipe

















# Regularization 

## what

* 防止过拟合
* 减少泛化误差，而不是训练误差；


## how

### 1. 参数范数惩罚

![](2.提升深度升级网络-超参数调节,%20正则化，优化/正则化_01.png)
* 正则项变大，然后w就会变小，激活函数如果是sigmoid就是趋向于中间，那么是个线性神经元，你的网络将只能学习到线性函数。
* 参数范数 Ω 的不同选择会导致不同的优先解。
* 我们只约束w，因为w表示两个变量的相互作用关系，而每个偏置仅控制一个单变量。

#### 1.1 L2正则化

* 这里学习率更新的部分没变， 每次更新前对权值进行了衰减。
* l2约束
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L2_正则化_01.png)
* 约束之后的目标函数
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L2_正则化_02.png)
* 约束对每个权值的影响
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L2_正则化_03.png)
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L2_正则化_04.png)
* 对于目标函数不明感的轴会随着训练慢慢衰减掉。
* 线性回归中L2正则化
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L2_正则化_05.png)
* L2 正则化不会导致参数变得稀疏。


#### 1.2 L1参数正则化

![](2.提升深度升级网络-超参数调节,%20正则化，优化/L1_正则化_01.png)
* L1 正则的过程
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L1_正则化_02.png)
* 相比 L2正则化， L1正则化会产生更稀疏的解。此处稀疏性指的是一些参数具有 0 的最优值。
 L1正则化的稀疏性是相比 L2正则化是质的不同。


### 2. 作为约束的范数惩罚

* 我们可以把参数惩罚看作对权重强加的约束。如果Ω是L2范数，那么权重就是被约束在一个L2球中，
如果Ω是L1范数，那么权重就是被约束在一个L1范数限制的区域中。
* 使用显式约束和重投影的优点
![](2.提升深度升级网络-超参数调节,%20正则化，优化/L1_正则化_03.png)


### 3 正则化和欠约束问题

* 正则化的必要性
![](2.提升深度升级网络-超参数调节,%20正则化，优化/正则化_02.png)


### 4. 数据增强

* 加入噪声数据
* 合理变换数据

### 5. 噪声鲁棒性

* 对于某些模型，在模型的输入加上方差极小的噪声等价于对权重施加范数惩罚。 
* 在一般情况下，噪声注入远比简单收缩参数强大，特别是噪声添加到隐藏单元时更加强大。

### 8 提前终止

* 验证是记录最小的误差下的权值
* 提前终止算法
![](2.提升深度升级网络-超参数调节,%20正则化，优化/提前终止.png)
* 提前终止的策略
![](2.提升深度升级网络-超参数调节,%20正则化，优化/提前终止_02.png)
* 第一种策略，再次初始化模型，然后所有数据训练，使用第一轮训练的最佳步数
![](2.提升深度升级网络-超参数调节,%20正则化，优化/提前终止_03.png)
* 第二种策略：第二次训练使用全部数据和第一轮得到的参数。然后监控验证集上的损失函数，作为停止条件
![](2.提升深度升级网络-超参数调节,%20正则化，优化/提前终止_04.png)

* 提前终止如果充当正则化


### 9 参数绑定和参数共享

* 我们之前讨论约束和惩罚时，都是相当于固定的区域和点，L2正则化固定对参数偏离零的固定值进行惩罚。
![](2.提升深度升级网络-超参数调节,%20正则化，优化/参数共享_01.png)
* 目前最流行的和广泛使用的参数共享出现在计算机视觉领域，cnn


### 10 稀疏表示

* 权重衰减施加直接作用于模型参数的惩罚，另一种策略是将惩罚放在神经网络的激活单元，估计对应的激活的稀疏。
* 表示的稀疏和参数的稀疏不一样
![](2.提升深度升级网络-超参数调节,%20正则化，优化/稀疏表示_01.png)


### 11. bagging和其他集成方法

* bagging：集合几个模型，
* 模型平均奏效的原因是不同的模型通常不会再测试机上产生完全相同的错误。


### 12 dropout

* dropout的目标是在指数级数量的神经玩过上近似这个过程；
* dropout训练与bagging训练不太一样，在bagging的情况下，所有模型是独立的，dropout情况下，模型是共享参数的，
其中每个模型继承的父神经网络参数的不同子集。
* 参数共享使得在有限可用的内存下代表指数数量的模型变得可能。
![](2.提升深度升级网络-超参数调节,%20正则化，优化/dropout_01.png)

* bagging集成必须从所有成员的累积投票做一个预测。我们将这个过程称为推断。
* dropout的方式不能依赖于任何的特征，因为任何特征都有可能被丢弃，
* dropOut是用来防止过拟合，所以除非你的网络过拟合， 负责不要使用。


















## optimization

* 寻找神经网络中的一组参数，它能显著降低代价函数，该代价函数包含整个训练集的评估和额外的正则项。
![](2.提升深度升级网络-超参数调节,%20正则化，优化/优化_01.png)

### normlize 数据

* 中心化
    * 计算均值
    * 减去均值
* 归一化
    * 计算方差：  δ**2 = 1/m * Σ(x{i} ** 2)
    * 减去方差： x / δ**2
* 这样你的数据分布更趋向于圆， 如果一直按着法线反向，能够更快得到最优结果。


### 梯度消失和爆炸

*  


### 批算法和minibatch

* 




## Build your first system quickly, then iterate

* 当你开始一个新项目时，尤其是在一个你不是专家的领域，很难正确猜测出最有前景的方向。
* 所以，不要在开始试图设计和构建一个完美的系统。相反，应尽可能快（可能在短短几天内）的构建和训练一个基本系统。
然后使用错误分析去帮助你识别最有前景的方向，并从那迭代地改进你的算法。
* 通过手动检查约100个算法错误分类的开发集样本来执行错误分析，并计算主要的错误类别。用这些信息来确定优先修正哪种类型的错误。
* 考虑将开发集分为手动检查的 Eyeball 开发集和不手动检查的 Blackbox 开发集。
如果在 Eyeball 开发集上的性能比在 Blackbox 开发集上好很多，那么你已经过拟合 Eyeball 开发集，并且应该考虑为其获得更多的数据。
* Eyeball 开发集应该足够大，以便于算法有足够多的错分类样本供你分析。对很多应用来说，含有1000-10000个样本的 Blackbox 开发集已足够。
* 如果你的开发集不够大到可以按照这种方式进行拆分，那么就使用 Eyeball 开发集来用于手动错误分析、模型选择和调超参。