## who

### reference

* [何恺明大神的「Focal Loss」，如何更好地理解？](https://zhuanlan.zhihu.com/p/32423092)
* [Focal loss论文详解](https://zhuanlan.zhihu.com/p/49981234)
* ICCV2017 **RBG和Kaiming大神的新作**
* [Focal Loss--从直觉到实现](https://mp.weixin.qq.com/s?__biz=MzIzNjc0MTMwMA==&mid=2247494120&idx=1&sn=5d714889b4901197cd4311c52c44d356&chksm=e8d19e33dfa61725d4141f8da7ad2acb0091ee05bcf508a9ed05606cdf67a8404a88ec80d20e&mpshare=1&scene=1&srcid=&sharer_sharetime=1581322887340&sharer_shareid=1fe097d88d780a66d1ae50b19263944c&key=48e002816d05900e7eed446734ee25208ccefefe93cfd1cd6f8ec5e8cb9d2114675814deaea5b8a355c610dbfe9e0f73fe93cb42e28b618a52150bfa4552bfdfc482057d7ee003e6696314c0e178906e&ascene=1&uin=MjY4MjcxNTEyNQ%3D%3D&devicetype=Windows+10&version=6208006f&lang=zh_CN&exportkey=A5RP3wczho0xgtM4hC7qvO8%3D&pass_ticket=kfUK9%2B9Oio1nT69Ryz1RphasxeOwc%2BeuuPdjQxBYRgTN5aKQGXLyKyvz6J4oBa1H)

* [paper](paper/21.001-18-Focal-Loss-for-Dense-Object-Detection.pdf)

## what



## where

### 目标

我们知道object detection的算法主要可以分为两大类：**two-stage detector和one-stage detector**。前者是指类似Faster RCNN，RFCN这样需要region proposal的检测算法，这类算法可以达到很高的准确率，但是速度较慢。虽然可以通过减少proposal的数量或降低输入图像的分辨率等方式达到提速，但是速度并没有质的提升。后者是指类似YOLO，SSD这样不需要region proposal，直接回归的检测算法，这类算法速度很快，但是准确率不如前者。**作者提出focal loss的出发点也是希望one-stage detector可以达到two-stage detector的准确率，同时不影响原有的速度。**



### 动机

* 既然有了出发点，**那么就要找one-stage detector的准确率不如two-stage detector的原因，作者认为原因是：样本的类别不均衡导致的**。我们知道在object detection领域，一张图像可能生成成千上万的candidate locations，但是其中只有很少一部分是包含object的，这就带来了类别不均衡。那么类别不均衡会带来什么后果呢？引用原文讲的两个后果：
  * **(1) training is inefficient as most locations are easy negatives that contribute no useful learning signal;**
  * **(2) en masse, the easy negatives can overwhelm training and lead to degenerate models.**

* 什么意思呢？**负样本数量太大，占总的loss的大部分，而且多是容易分类的，因此使得模型的优化方向并不是我们所希望的那样**。其实先前也有一些算法来处理类别不均衡的问题，比如OHEM（online hard example mining），OHEM的主要思想可以用原文的一句话概括：In OHEM each example is scored by its loss, non-maximum suppression (nms) is then applied, and a minibatch is constructed with the highest-loss examples。**OHEM算法虽然增加了错分类样本的权重，但是OHEM算法忽略了容易分类的样本。**

* 因此针对类别不均衡问题，作者提出一种新的损失函数：focal loss，这个损失函数是在标准交叉熵损失基础上修改得到的。**这个函数可以通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。**为了证明focal loss的有效性，作者设计了一个dense detector：RetinaNet，并且在训练时采用focal loss训练。**实验证明RetinaNet不仅可以达到one-stage detector的速度，也能有two-stage detector的准确率。**



## how



### 交叉熵损失

* 介绍focal loss，在介绍focal loss之前，先来看看交叉熵损失，这里以二分类为例，**原来的分类loss是各个训练样本交叉熵的直接求和，也就是各个样本的权重是一样的**。公式如下：

  ![v2-ced281bd89909a8c04a47c257b890b85_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-交叉熵损失.jpg)

因为是二分类，p表示预测样本属于1的概率（范围为0-1），y表示label，y的取值为{+1,-1}。当真实label是1，也就是y=1时，假如某个样本x预测为1这个类的概率p=0.6，那么损失就是-log(0.6)，注意这个损失是大于等于0的。如果p=0.9，那么损失就是-log(0.9)，所以p=0.6的损失要大于p=0.9的损失，这很容易理解。这里仅仅以二分类为例，多分类分类以此类推

* 为了方便，用pt代替p，如下公式2:。这里的pt就是前面Figure1中的横坐标。

  ![v2-2814547ad818bea2b21422f2efa76947_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-交叉熵损失-02.jpg)

为了表示简便，我们用p_t表示样本属于true class的概率。所以(1)式可以写成

![v2-5e2b1cd0321d065768803f13799ee341_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-交叉熵损失-03.jpg)

### 最基本交叉熵的改进

* 接下来介绍一个**最基本的对交叉熵的改进，也将作为本文实验的baseline**，既然one-stage detector在训练的时候正负样本的数量差距很大，那么一种常见的做法就是给正负样本加上权重，负样本出现的频次多，那么就降低负样本的权重，正样本数量少，就相对提高正样本的权重。**因此可以通过设定a的值来控制正负样本对总的loss的共享权重。a取比较小的值来降低负样本（多的那类样本）的权重。**

* **显然前面的公式3虽然可以控制正负样本的权重，但是没法控制容易分类和难分类样本的权重**，于是就有了focal loss：

  ![v2-80fe6365f9adbd28c7e8f7e82ff4a711_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-交叉熵损失-改进-01.jpg)

  这里的γ称作focusing parameter，γ>=0。

  ![v2-bcd7168b9a782f8e0dc32f834934b366_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-交叉熵损失-改进-02.jpg)

称为调制系数（modulating factor）



为什么要加上这个调制系数呢？**目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本。**



* **绘制图看如下Figure1**，横坐标是pt，纵坐标是loss。CE（pt）表示标准的交叉熵公式，FL（pt）表示focal loss中用到的改进的交叉熵。**Figure1中γ=0的蓝色曲线就是标准的交叉熵损失。**

![v2-8b51156398d097da506c524534026fa9_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-交叉熵损失-改进-03.jpg)

### focal loss的两个重要性质：

1、当一个样本被分错的时候，pt是很小的，那么调制因子（1-Pt）接近1，损失不被影响；当Pt→1，因子（1-Pt）接近0，那么分的比较好的（well-classified）样本的权值就被调低了。因此调制系数就趋于1，也就是说相比原来的loss是没有什么大的改变的。当pt趋于1的时候（此时分类正确而且是易分类样本），调制系数趋于0，也就是对于总的loss的贡献很小。

**2、**当γ=0的时候，focal loss就是传统的交叉熵损失，**当γ增加的时候，调制系数也会增加。** 专注参数γ平滑地调节了易分样本调低权值的比例。γ增大能增强调制因子的影响，实验发现γ取2最好。直觉上来说，调制因子减少了易分样本的损失贡献，拓宽了样例接收到低损失的范围。当γ一定的时候，比如等于2，一样easy example(pt=0.9)的loss要比标准的交叉熵loss小100+倍，当pt=0.968时，要小1000+倍，但是对于hard example(pt < 0.5)，loss最多小了4倍。这样的话hard example的权重相对就提升了很多。这样就增加了那些误分类的重要性

* **focal loss的两个性质算是核心，其实就是用一个合适的函数去度量难分类和易分类样本对总的损失的贡献。**
* **作者在实验中采用的是公式5的focal loss（结合了公式3和公式4，这样既能调整正负样本的权重，又能控制难易分类样本的权重）：**

![v2-c64032cad48b6e151e4df7cd5fef4edc_hd](readme/21.001-Focal-Loss-for-Dense-Object-Detection-focal-loss-公式.jpg)



## how much

Figure2是在COCO数据集上几个模型的实验对比结果。可以看看再AP和time的对比下，本文算法和其他one-stage和two-stage检测算法的差别。

![v2-1c1ec17150b2fec0b8847bbfc16b7616_r](readme/21.001-Focal-Loss-for-Dense-Object-Detection-实验-coco.jpg)