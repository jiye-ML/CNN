[计算图（computational graph）角度看BP（back propagation）算法](https://blog.csdn.net/u013527419/article/details/70184690)

- 还是动态规划的思路。

* 前向传播
* 本层的输入和前一层输出有关
![](readme/20.80-02-Foward.png)

* 反向传播
* 本层的输入和后一个层的输出有关
![](readme/20.80-02-backward.png)

## 一、通用形式

### 1. 什么是计算图结构 
从下图中我们可以清楚地看到 
（1）可以将计算图看作是一种用来描述function的语言，图中的节点node代表function的输入（可以是常数、向量、张量等），图中的边代表这个function操作。 
（2）看到一个计算图的例子，我们就可以很容易地写出其对应的function。 

![img](readme/20.80-02-计算图.png)

### 2.用计算图和链式法则来算偏导数

从下图中中的计算图我们可以得到它对应的function：e = (a+b) ∗ (b+1)，若我们要求e对b的偏导数![这里写图片描述](https://img-blog.csdn.net/20170415135202819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUyNzQxOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)，可经过以下步骤求得：

（1）图中绿色部分计算每一条edge上的偏导数，easy。 
（2）从e到参数b有两条路径，把这两条路径上计算出的偏导数相乘，最后相加即为所求目标。（图中红色箭头）

![è¿éåå¾çæè¿°](readme/20.80-02-计算图-02.png)

举个例子： 
若已知a = 3, b = 2，求e对b的偏导数： 
（1）首先，前向计算很容易得到c，d，e的值（下图中橘红色的部分，很容易） 
（2）算出每条路径上的偏导数值（下图中黄色部分，很容易） 

（3）从节点b开始找到从自身节点到目标节点e有两条路径，每条路径上偏导数相乘后的结果为1*3和1*5，最后相加得到结果8。

![è¿éåå¾çæè¿°](readme/20.80-02-计算图-03.png)

### 3.如果要求同时求一个参数对多个参数的偏导数呢？ 
例如下图中要求同时求e对a，b的偏导数： 
法一：按照2中的步骤分别求出来即可，这样子的话如果参数很多，分别求就很没效率。（称为 forward mode） 
法二：可以反方向来求，如下图中绿色箭头，这样遍历一次图就可以同时求出来，更快。（称为 reverse mode） 

此法适用于有一个root的结构，例如在神经网络中，我们用到就是reverse mode，因为在NN中，我们要求的就是最终的loss function的偏微分，这个function的输出就只有一个，要算这个loss function对所有参数的偏微分，反向更有效率。 

![è¿éåå¾çæè¿°](readme/20.80-02-计算图-04.png)

### 4.如果计算图中有Parameter sharing（即同一个参数出现在多个不同的node上）的状况呢？ 
Parameter sharing也是NN中经常发生的事，例如CNN,RNN中都有。 
步骤： 
（1）计算每一条边上的偏微分，如下图中绿色箭头，注意计算中将相同的参数看作不同的对待，如将x看作带不同下标的x== 
（2）将y到三个x的三条路径的偏微分之乘相加即可。

即遇到Parameter sharing的情况时，先将同一个参数相当做不同的参数来看，算完之后再相加。 

![è¿éåå¾çæè¿°](readme/20.80-02-计算图-05.png)

## 二、全连接神经网络中的计算图 

**1.先复习一下bp算法** 

简而言之，如下， 
第一个红色框中的部分第l层的第i个神经元对与其相连的j个权重的偏导数分别为l-1层中每一个权重参数w连接的那个神经元的输出，这个通过一个前向传播就可以很容易求出所有的偏导数值。 

第二个绿色框中的部分loss function对第l层的神经元的输入z的偏导可以通过将误差后向递归传播得到。

![è¿éåå¾çæè¿°](readme/20.80-02-BP-01.png)

**2.用计算图来做bp**

![è¿éåå¾çæè¿°](readme/20.80-02-用计算图来做BP.png)

在计算的过程中，可见要用到一个向量y对另一个向量x的偏导数，此处用雅各布矩阵表示。矩阵的长为x的维数，高为y的维数，如下所示。

![è¿éåå¾çæè¿°](readme/20.80-02-用计算图来做BP-02.png)

所以接下来依次计算计算图中每一条边上的偏导数。 
（1）首先计算loss function对输出y的偏导数。一个常量C对一个向量y的偏导数为长为y的维数，高为1（C的维数）。下图中以分类问题为例，用的loss function 为交叉熵。

![è¿éåå¾çæè¿°](readme/20.80-02-用计算图来做BP-03.png)

![image-20190531202044285](readme/20.80-02-用计算图来做BP-04.png)

![image-20190531202127160](readme/20.80-02-用计算图来做BP-05.png)

![image-20190531202156658](readme/20.80-02-用计算图来做BP-06.png)

![image-20190531202225732](readme/20.80-02-用计算图来做BP-07.png)

![image-20190531203453472](readme/20.80-02-问题.png)