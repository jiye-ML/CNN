* [深度学习网络调参技巧](https://zhuanlan.zhihu.com/p/24720954)
    * 好的实验环境是成功的一半
        * 将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。
        * 可以输出模型的损失函数值以及训练集和验证集上的准确率。
        * 可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。
        再由一个主程序，分配参数以及并行启动一系列子程序。
    * 画图
    * 从粗到细分阶段调参
        * 建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。
        * 如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，
        得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，
        学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。
    * 提高速度: 调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。
    因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。
        * 对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。
        * 减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。
    * 经验参数 
        * learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。
        衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。 
        不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，
        可以避免再费劲调节学习率。那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。
        
        
* [深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837)

### 初始化

* [github 初始化总结](https://github.com/jiye-ML/DeepLearning-study/blob/master/11.%E5%88%9D%E5%A7%8B%E5%8C%96.md)
* [CS231n课程笔记翻译：神经网络笔记 2](https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit)
    * 归一化（Normalization）:是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。
        第一种是先对数据做零中心化处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。
        第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。
    * PCA和白化（Whitening）是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。

### 数据预处理

* zero-center ,这个挺常用的. X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize
* PCA whitening,这个用的比较少.

* [你有哪些deep learning（rnn、cnn）调参的经验-罗浩.ZJU](https://www.zhihu.com/question/41631631/answer/94816420)
    * relu+bn。这套好基友组合是万精油: 可以满足95%的情况，除非有些特殊情况会用identity，比如回归问题，
    比如resnet的shortcut支路，sigmoid什么的都快从我世界里消失了；
    * dropout 。分类问题用dropout ，只需要最后一层softmax 前用基本就可以了，能够防止过拟合，可能对accuracy提高不大，
    但是dropout 前面的那层如果是之后要使用的feature的话，性能会大大提升；
    * 数据的shuffle 和augmentation 。这个没啥好说的，aug也不是瞎加，比如行人识别一般就不会加上下翻转的，因为不会碰到头朝下的异型种；
    * 降学习率。随着网络训练的进行，学习率要逐渐降下来，如果你有tensorboard，你有可能发现，在学习率下降的一瞬间，
    网络会有个巨大的性能提升，同样的fine-tuning也要根据模型的性能设置合适的学习率，，
    比如一个训练的已经非常好的模型你上来就1e-3的学习率，那之前就白训练了，就是说网络性能越好，学习率要越小；
    * tensorboard。以前不怎么用，用了之后发现太有帮助，帮助你监视网络的状态，来调整网络参数；
    * 随时存档模型，要有validation 。这就跟打游戏一样存档，把每个epoch和其对应的validation 结果存下来，
    可以分析出开始overfitting的时间点，方便下次加载fine-tuning；
    * 网络层数，参数量什么的都不是大问题，在性能不丢的情况下，减到最小；
    * batchsize通常影响没那么大，塞满卡就行，除了特殊的算法需要batch大一点；
    * 输入减不减mean归一化在有了bn之后已经不那么重要了；
    * 卷积核的分解。从最初的5×5分解为两个3×3，到后来的3×3分解为1×3和3×1，再到resnet的1×1，3×3，1×1，
    再xception的3×3 channel-wise conv+1×1，网络的计算量越来越小，层数越来越多，性能越来越好，这些都是设计网络时可以借鉴的；
    * 不同尺寸的feature maps的concat，只用一层的feature map一把梭可能不如concat好，pspnet就是这种思想，这个思想很常用；
    * resnet的shortcut确实会很有用，重点在于shortcut支路一定要是identity，主路是什么conv都无所谓，这是我亲耳听resnet作者所述

* [关于神经网络的调参顺序?](https://www.zhihu.com/question/29641737)

* [Hyperparameter optimization for Neural Networks](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)
    * 需要优化的参数：
        1. Number of layers
        2. Different parameters for each layer (number of hidden units, filter size for convolutional layer and so on)
        3. Type of activation functions
        4. Parameter initialization method
        5. Learning rate
        6. Loss function
    * 搜索策略：
        1. Grid Search
        2. Random Search
        3. Hand-tuning
        4. Gaussian Process with Expected Improvement
        5. Tree-structured Parzen Estimators (TPE)