* [深度学习网络调参技巧](https://zhuanlan.zhihu.com/p/24720954)
* [深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837)

### 初始化

* [github 初始化总结](https://github.com/jiye-ML/DeepLearning-study/blob/master/11.%E5%88%9D%E5%A7%8B%E5%8C%96.md)
* [CS231n课程笔记翻译：神经网络笔记 2](https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit)
    * 归一化（Normalization）:是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。
        第一种是先对数据做零中心化处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。
        第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。
    * PCA和白化（Whitening）是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。

### 数据预处理

* zero-center ,这个挺常用的. X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize
* PCA whitening,这个用的比较少.

* [你有哪些deep learning（rnn、cnn）调参的经验-罗浩.ZJU](https://www.zhihu.com/question/41631631/answer/94816420)
    * relu+bn。这套好基友组合是万精油: 可以满足95%的情况，除非有些特殊情况会用identity，比如回归问题，
    比如resnet的shortcut支路，sigmoid什么的都快从我世界里消失了；
    * dropout 。分类问题用dropout ，只需要最后一层softmax 前用基本就可以了，能够防止过拟合，可能对accuracy提高不大，
    但是dropout 前面的那层如果是之后要使用的feature的话，性能会大大提升；
    * 数据的shuffle 和augmentation 。这个没啥好说的，aug也不是瞎加，比如行人识别一般就不会加上下翻转的，因为不会碰到头朝下的异型种；
    * 降学习率。随着网络训练的进行，学习率要逐渐降下来，如果你有tensorboard，你有可能发现，在学习率下降的一瞬间，
    网络会有个巨大的性能提升，同样的fine-tuning也要根据模型的性能设置合适的学习率，，
    比如一个训练的已经非常好的模型你上来就1e-3的学习率，那之前就白训练了，就是说网络性能越好，学习率要越小；
    * tensorboard。以前不怎么用，用了之后发现太有帮助，帮助你监视网络的状态，来调整网络参数；
    * 随时存档模型，要有validation 。这就跟打游戏一样存档，把每个epoch和其对应的validation 结果存下来，
    可以分析出开始overfitting的时间点，方便下次加载fine-tuning；
    * 网络层数，参数量什么的都不是大问题，在性能不丢的情况下，减到最小；
    * batchsize通常影响没那么大，塞满卡就行，除了特殊的算法需要batch大一点；
    * 输入减不减mean归一化在有了bn之后已经不那么重要了；
    * 卷积核的分解。从最初的5×5分解为两个3×3，到后来的3×3分解为1×3和3×1，再到resnet的1×1，3×3，1×1，
    再xception的3×3 channel-wise conv+1×1，网络的计算量越来越小，层数越来越多，性能越来越好，这些都是设计网络时可以借鉴的；
    * 不同尺寸的feature maps的concat，只用一层的feature map一把梭可能不如concat好，pspnet就是这种思想，这个思想很常用；
    * resnet的shortcut确实会很有用，重点在于shortcut支路一定要是identity，主路是什么conv都无所谓，这是我亲耳听resnet作者所述

* [关于神经网络的调参顺序?](https://www.zhihu.com/question/29641737)

* [Hyperparameter optimization for Neural Networks](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)
    * 需要优化的参数：
        1. Number of layers
        2. Different parameters for each layer (number of hidden units, filter size for convolutional layer and so on)
        3. Type of activation functions
        4. Parameter initialization method
        5. Learning rate
        6. Loss function
    * 搜索策略：
        1. Grid Search
        2. Random Search
        3. Hand-tuning
        4. Gaussian Process with Expected Improvement
        5. Tree-structured Parzen Estimators (TPE)