
* [如何直观地解释 back propagation 算法？](https://www.zhihu.com/question/27239198?rf=24827633)
    * 其主要目的是通过将输出误差反传，将误差分摊给各层所有单元，从而获得各层单元的误差信号，
    进而修正各单元的权值（其过程，是一个权值调整的过程）。
    
* [计算图（computational graph）角度看BP（back propagation）算法](https://blog.csdn.net/u013527419/article/details/70184690)
    * 还是动态规划的思路。
    
* [Backpropagation In Convolutional Neural Networks](http://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)
* [http://cs231n.github.io/optimization-2/](http://cs231n.github.io/optimization-2/)

* [深入解析Backpropagation反向传播算法](https://liuchi.coding.me/2018/01/26/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Backpropagation%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/)
    * Backpropagation反向传播算法的本质就是链式法则 和 动态规划!!!


## 梯度弥散和梯度爆炸

### 原因分析

* 通常神经网路用的激活函数是sigmoid，而sigmoid函数能将负无穷大到正无穷大的数据映射到0-1之间，并且这个函数的导数
f'(x)=f(x)*(1-f(x))。因此两个0-1之间的数相乘，得到的结果就会变得很小，反向传播是逐层对函数偏导数相乘。因此，
当网络层数非常深的时候，最后一层产生的偏差就因为成了很多小于1的数二越来越小，最终为0，从而导致，浅层的权重没有更新。
* 梯度爆炸是因为初始化权重过大，前面层会比后面层变化的更快。

* [梯度弥散与梯度爆炸](https://www.cnblogs.com/yangmang/p/7477802.html)
* [详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526)
    * 对于本层来说：如果 `z=f(wa2 + b2)`，则导数为 `dz = (df/dw)*w*da2`,
        * 如果 `(df/dw)*w`很大就会使得`dz`很大，
        * 如果`(df/dw)*w`很小就会使得`dz`很小，出现不学习的情况，更新很慢，需要更大的学习率。
    * 对于梯度反向回传来说：过小的`dz`，会使得梯度消失，过大的`dz`会使得梯度爆炸；这里`(df/dw)`可以理解为激活函数的
    导数，如果使用sigmod，那么双端饱和，梯度很小，dz自然会很小，层数加深，就会消失。
    
* [bp算法中为什么会产生梯度消失？](https://www.zhihu.com/question/49812013)
  
* [Why are deep neural networks hard to train?](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem)



### 解决方案

#### ReLU激活函数

* 用ReLU替代sigmoid函数，ReLU>0的区域不会饱和；ReLU有一定的稀疏性，主要解决梯度消失问题。
    * 优点： 使用 ReLU得到的SGD的收敛速度会比 sigmoid/tanh 快。这是因为它是linear，而且ReLU只需要一个阈值就可以得到激活值，
    不用去计算复杂的运算。
    * 缺点： 训练过程该函数不适应较大梯度输入，因为在参数更新以后，ReLU的神经元不会再有激活的功能，导致梯度永远都是零。
    * 为了针对以上的缺点，又出现Leaky-ReLU、P-ReLU、R-ReLU三种拓展激活函数。
* 个人理解：
    * 主要是计算方便，计算快，带了的一半的神经元失活问题，应该是网络本身的冗余，之后有些论文就像神经网络压缩了，
    比如VGG压缩到了很小，


#### Batch normalization层

* Batch normalizatinon层：每次SGD的时候，通过mini-batch对相应的activation做规范化操作，使得输入都为均为值0，反差为1，
这样每一层满足同样的相似分布，而且BN中有两个参数，可以控制均值和方差；
* 反向传播式子中有w的存在，所以 w 的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，
通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布，
即严重偏离的分布强制拉回比较标准的分布，这样**使得激活输入值落在非线性函数对输入比较敏感的区域**，
这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，
能大大加快训练速度。


#### -残差结构

![梯度消失-爆炸-解决思路-残差-损失函数](readme/梯度消失-爆炸-解决思路-残差.jpg)