## 写在前面

> 由于batch norm属于inception v2，然后将这部分正则内容放在
https://github.com/jiye-ML下的01里，请具体参见；

* 正则化是为了不让模型太复杂而对权值的进行的约束，使得权值在一定范围内，

## 手写笔记

![手写笔记](readme/正则化_手写笔记_01.jpg)
![手写笔记](readme/正则化_手写笔记_02.jpg)
![手写笔记](readme/正则化_手写笔记_03.jpg)
![手写笔记](readme/正则化_手写笔记_04.jpg)
![手写笔记](readme/正则化_手写笔记_05.jpg)
![手写笔记](readme/正则化_手写笔记_06.jpg)

# Regularization 

## what

* 防止过拟合
* 减少泛化误差，而不是训练误差；


## how
* [code](readme/Regularization.html)

### 1. 参数范数惩罚

#### 1.1 L2正则化

* 约束对每个权值的影响
![](readme/正则化_L2_04.png)
* 对于目标函数不敏感的轴会随着训练慢慢衰减掉。
* 线性回归中L2正则化
* L2 正则化不会导致参数变得稀疏。


#### 1.2 L1参数正则化

* 相比 L2正则化， L1正则化会产生更稀疏的解。此处稀疏性指的是一些参数具有 0 的最优值。
 L1正则化的稀疏性是相比 L2正则化是质的不同。


### 2. 作为约束的范数惩罚

* 我们可以把参数惩罚看作对权重强加的约束。如果Ω是L2范数，那么权重就是被约束在一个L2球中，
如果Ω是L1范数，那么权重就是被约束在一个L1范数限制的区域中。
* 使用显式约束和重投影的优点
![](readme/正则化_L1_03.png)


### 3 正则化和欠约束问题

* 正则化的必要性
![](readme/正则化_02.png)


### 4. 数据增强

* 加入噪声数据
* 合理变换数据

### 5. 噪声鲁棒性

* 对于某些模型，在模型的输入加上方差极小的噪声等价于对权重施加范数惩罚。 
* 在一般情况下，噪声注入远比简单收缩参数强大，特别是噪声添加到隐藏单元时更加强大。

### 8 提前终止

* 验证是记录最小的误差下的权值
* 提前终止如果充当正则化


### 9 参数绑定和参数共享

* 我们之前讨论约束和惩罚时，都是相当于固定的区域和点，L2正则化固定对参数偏离零的固定值进行惩罚。
![](readme/参数共享_01.png)
* 目前最流行的和广泛使用的参数共享出现在计算机视觉领域，cnn


### 10 稀疏表示

* 权重衰减施加直接作用于模型参数的惩罚，另一种策略是将惩罚放在神经网络的激活单元，估计对应的激活的稀疏。
* 表示的稀疏和参数的稀疏不一样
![](readme/稀疏表示_01.png)


### 11. bagging和其他集成方法

* bagging：集合几个模型，
* 模型平均奏效的原因是不同的模型通常不会再测试机上产生完全相同的错误。


### 12 dropout

* [dropout](02.dropout.md)



### 额外主题： 学习率衰减


### 额外主题：softmax

* 输入和输出都是向量，只是输出进行了归一化处理
* softmax的损失函数自然是交叉熵

### 番外篇

* [概览深度学习中的五大正则化方法和七大优化策略](https://zhuanlan.zhihu.com/p/32194445)
    * 

* [机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995)
    * 最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据;
    * L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。
    * L1范数是指向量中各个元素绝对值之和，
    * L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），
    有人也叫它“权值衰减weight decay”; 
    * L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。\
    ![](readme/正则化_03.png)
    * 加入正则项对于函数曲线的影响： 左边是加入后的曲线，右边是加入前的曲线 \
    ![](readme/正则化_04.png)
    
* [机器学习中的范数规则化之（二）核范数与规则项参数选择](https://blog.csdn.net/zouxy09/article/details/24972869)
    * 
    


## 杂谈

### Yoshua Bengio

* 你要设法自己实现这些功能, 尽管可能效率不高 但是仅仅是为了确保你真正理解背后的东西 这点非常有用 自己多尝试
* 也就是说不要只用那些编程框架, 让你可以用几行代码 就完成所有功能, 但你实际不知道底层原理.
* 我想说我们应该更进一步 如果可以的话, 设法自己从基本原理中推出这些东西 这真的很有帮助 但是通常情况下 你必须阅读
 看其他人的代码 写自己的代码 做很多试验, 确保你理解你做的所有事情 特别对于科学来说 这是其中的一部分 
* 问问自己为什么我要做这些, 为什么其他人在做这些 可能答案就在书本的某一页, 你必须读更多的书  
如果你实际上能自己想出来, 那就更好  对 这样很酷
* 不过ICLRI大会汇刊 收集的好论文可能最多 当然NIPS和ICML 还有其他的会议, 也有很好的论文 但是如果你真的想接触很多好论文,
只要阅读最近几期的 ICLR汇刊, 这能让你真正地看清这个领域
* 这取决于你从哪儿开始 不要害怕数学 只要发展直觉， 然后只要你从直觉上把握了 事物背后的原理 数学就会变得相当容易
* 还有个好消息是你不需要5年的博士学习 来成为深度学习的高手 实际上如果你有很好的计算机科学和数学基础 只要学习短短几个月
 你就可以用好它 构建出东西 并且开始做研究试验 如果接受过良好的训练 大概只要六个月 可能他们一点都不了解 机器学习 
* 但是 如果他们擅长数学和计算机科学 这个过程会很快 当然 这意味着在数学和计算机科学方面 你需要有良好的训练 有时候
 你在计算机科学课程中学到的还不够 特别是 需要补充一些数学 比如概率论 代数和优化。 



### jiye

* 要对你所学习的东西有一个清晰的认识，这样才能学好。
* 正则化和优化说的是如何更好的拟合你的数据
    * 正则化：如何更好拟合训练集和测试集
    * 优化：如何更好拟合训练集，降低误差

