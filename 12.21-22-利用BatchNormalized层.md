#### Batch normalization层

- Batch normalizatinon层：每次SGD的时候，通过mini-batch对相应的activation做规范化操作，使得输入都为均为值0，反差为1，
  这样每一层满足同样的相似分布，而且BN中有两个参数，可以控制均值和方差；
- 反向传播式子中有w的存在，所以 w 的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出做scale和shift的方法，
  通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布，
  即严重偏离的分布强制拉回比较标准的分布，这样**使得激活输入值落在非线性函数对输入比较敏感的区域**，
  这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，
  能大大加快训练速度。

