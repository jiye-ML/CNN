---
typora-copy-images-to: readme
---

## 写在前面

## 传统经典方法

* [基于内容的图像检索技术综述-传统经典方法](https://mp.weixin.qq.com/s/Sda94q-40goiZGSYGgm_Yw

    * 早期的图片检索技术都是基于文本的，需要按照图片的名称去搜索对应的图片，而这样有个很明显的缺陷就是：大量的图片需要人为事先去命名，这个工作量太大了。随后渐渐出现了基于内容的图片检索技术，较早出现的有哈希算法LSH(Locality-Sensitive Hashing)，随后图搜这一块逐渐丰富，从BOF -> SPM -> ScSPm ->LLC 使传统的图搜技术逐渐成熟。

    * LSH (Locality-Sensitive Hashing) 

        * 将原始数据空间中的两个相邻数据点通过相同的映射后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小。 
        * （一）**平均哈希算法**(aHash)：此算法是基于比较灰度图每个像素与平均值来实现的，最适用于缩略图搜索，步骤：
            1. 缩放图片：为了保留结构去掉细节，去除大小、横纵比的差异，把图片统一缩放到8*8，共64个像素。
            2. 转化为灰度图：把缩放后的图片转化为256阶的灰度图；
            3. 计算平均值： 计算进行灰度处理后图片的所有像素点的平均值；
            4. 比较像素灰度值：遍历64个像素，如果大于平均值记录为1，否则为0.
            5. 得到信息指纹：组合64个bit位，顺序随意保持一致性即可。
            6. 对比指纹：计算两幅图片的汉明距离，汉明距离越大则说明图片越不一致，反之，汉明距离越小则说明图片越相似，当距离为0时，说明完全相同。(通常认为距离>10 就是两张完全不同的图片)

        * (二)、**感知哈希算法(pHash)** ：平均哈希算法过于严格，不够精确，更适合搜索缩略图，为了获得更精确的结果可以选择感知哈希算法，它采用的是DCT(离散余弦变换)来降低频率的方法，步骤：
            1. 缩小图片：32 * 32是一个较好的大小，这样方便DCT计算
            2. 转化为灰度图：把缩放后的图片转化为256阶的灰度图
            3. 计算DCT: DCT把图片分离成分率的集合
            4. 缩小DCT：DCT是32 * 32，保留左上角的8 * 8，这些代表的图片的最低频率
            5. 计算平均值：计算缩小DCT后的所有像素点的平均值
            6. 进一步减小DCT：大于平均值记录为1，反之记录为0
            7. 得到信息指纹：同平均哈希算法对比指纹：同平均哈希算法
        * (三)、**差异哈希算法( dHash)**： 相比pHash，dHash的速度要快的多，相比aHash，dHash在效率几乎相同的情况下的效果要更好，它是基于渐变实现的， 步骤：
            1. 缩小图片：收缩到9 * 8的大小，共72个像素点
            2. 转化为灰度图：把缩放后的图片转化为256阶的灰度图
            3. 计算差异值：dHash算法工作在相邻像素之间，这样每行9个像素之间产生了8个不同的差异，一共8行，则产生了64个差异值
            4. 获得指纹：如果左边像素的灰度值比右边高，则记录为1，否则为0
            5. 对比指纹：同平均哈希算法

    * BOW-> BOF

        * BOW(Bag of Words) 模型最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些不同类别词汇的集合，而文本中的每个词汇都是独立的。简单说就是将每篇文档都看成一个袋子，这个袋子里面装的是各种类别的词汇，我们按照类别把整篇文档的词汇归为不同的类，比如这些词汇的类可以是枪、银行、船、人、桌子等，然后依据每个类别中词汇出现的频率来判断整篇文档所描述的大致内容。比如一篇文档中枪、银行这两类词汇出现的概率比较高，我们就可以判断这篇文档和武装押运有关，或者是关于土匪抢银行的，兄台们可自行发挥自己的脑洞。 
        * 类比到图像就是BOF(Bag of Features)了，以上所述的“袋子”就相当于是一副完整的图像，而“词汇”则相当于图像的局部特征(如SIFT、SURF)，先用这些局部特征来训练出图像的聚类中心，训练聚类中心的过程即相当于按照类别把文档的词汇归为不同的类。在图片检索的时候，对图片的每一个局部特征用近邻查找法找到距离它最近的聚类中心，并把此聚类中心上局部特征的数目加一，依次遍历每一个局部特征后就把一副图片映射到一个聚类中心上，即图片的量化。最后以这些聚类中心为横坐标，以每个聚类中心的局部特征个数为纵坐标可以得到一个直方图，该直方图表示的向量就是一副图片映射到聚类中心的BOF向量。图片检索的时候只要依次比较图像的BOF向量即可找到最相似的图片。 ![BOF_示意图](G:\AI\DeepLearning-study\readme/BOF_示意图.png)

    * 指数权重VLAD(Vector of Aggragate Locally Descriptor) 

        * BOW是把局部特征的个数累加到聚类中心上，而VLAD是把**局部特征相对于聚类中心的偏差(有正负)**累加到聚类中心上，而且是对最相邻的k个聚类中心都进行累加(k一般设为4左右)，这样能很大程度地提高特征量化的准确度，而且还能减少聚类中心的数目以提高量化速度。在累加每一个局部特征的偏差时，实际上累加的不是一个数，而是一个局部特征向量，比如用SIFT特征时累加的就是一个128维的向量，这样最终VLAD向量的维度就是128*聚类中心个数。如果聚类中心个数是256，最终的VLAD向量就是32768维。用这么大的向量去表征一副图片，显然会显得冗余，所以我们对直接累加的VLAD向量还要进行PCA降维，作者在使用VLAD向量的时候把它降到了512维，识别速率有了质的提升而识别率却基本维持不变。 

        * 因为PCA降维矩阵是按照特征值从大到小排列的，所以经过PCA降维处理后特征向量的前几个数据所占的比重会比较大，要远大于平均值，如图6所示。这样对特征的提取会造成较大干扰，因为若是前几个数据出现了差错，其引起的数据波动也往往比较大，在比较特征向量相似度时就容易产生较大误差，所以理想情况是使特征向量中前几个过大的数据按一定比例缩小，而使后面变化不大的数据尽量保持不变。 

            ![PCA降维后的VLAD向量](G:\AI\DeepLearning-study\readme/PCA降维后的VLAD向量-1535938444168.png)

            通过观察特征向量直方图可以发现它在二维坐标上的分布类似于指数函数，如图7a)所示为指数函数f(x)=exp(-1)，所以考虑用图7b)所示的指数函数g(x)=1-exp(-x)作为权重和特征向量的每一个数据相乘。 ![指数函数](G:\AI\DeepLearning-study\readme/指数函数.png)

        * 但是在权重和数据相乘的时候还会有一个问题：当x取值很接近0的时候权重值g(x)也很接近0，当权重过小时会抹掉特征向量的前几个数据，这样会造成特征向量的部分数据无效，在度量特征向量相似度时反而会增大误差，所以在取离散g(x)值作权重的时候不能从0开始取值而应当有一个初始值。 

        * 将图6的特征向量和离散权重相乘可得到新的特征向量直方图，如图8所示，可见特征向量的前几个较大的数据被削减，而后续数据基本维持不变。 ![VALD_指数权重VLAD](G:\AI\DeepLearning-study\readme/VALD_指数权重VLAD.png)

        * 但是用VLAD向量做图片检索也存在很多缺点：首先，作为传统的图像识别方法，它需要手动提取特征，再加上K-means聚类时间长，会使得算法很繁琐；其次在向量量化的过程中会损失特征的精度，模板图片的设计也显得很粗糙，而且整个过程没有设计反馈系统，系统无法自动升级，迁移性很差。 

    *  **FV** 

        * FV(Fisher Vector)是一种类似于BOW词袋模型的一种编码方式，如提取图像的SIFT特征，通过矢量量化(K-Means聚类)，构建视觉词典(码本)，FV采用混合高斯模型(GMM)构建码本，但是FV不只是存储视觉词典的在一幅图像中出现的频率，并且FV还统计视觉词典与局部特征的差异。可见FV和VLAD的差别就是FV用GMM构建码本，而VLAD用K-Means构建码本。 
        * FV其实就是对于高斯分布的变量求偏导！也就是对权重、均值、标准差求偏导得到的结果，其本质上是用似然函数的梯度向量来表达一幅图像，这个梯度向量的物理意义就是数据拟合中对参数调优的过程，下面我们来说一下GMM。 

## CNN方法

* [基于内容的图像检索技术综述-CNN方法](https://mp.weixin.qq.com/s/P8s8Uee4ssU029pM6GfnDg)
    * 利用传统CNN网络提取特征

    * netVLAD

      * 相对于传统的人工设计特征，CNN已经在图像特征提取上显示出了强大的功力。在图像检索问题中，目前有基于全局和基于局部两种卷积神经网络特征表示方法。基于全局的方法直接使用卷积神经网络提取整幅图像的特征，作为最终的图像特征。但是因为卷积神经网络主要对全局空间信息进行编码，导致所得特征缺乏对图像的尺度、旋转、平移等几何变换和空间布局变化的不变性，限制了其对于高度易变图像检索的鲁棒性。对于基于局部的方法，使用卷积神经网络提取图像局部区域的特征（这里的局部区域特征好比经典方法的SIFT特征），然后聚合这些区域特征生成最终的图像特征。虽然这些方法考虑到了图像的局部信息，但仍有一些缺陷。例如使用滑动窗口来得到图像区域时，由于没有考虑到图像的颜色、纹理、边缘等视觉内容，会产生大量无语义意义的区域，为之后的聚合过程带来冗余和噪声信息。另外，区域特征融合通常所使用的最大池化算法，因只保留了特征的最大响应而没有考虑特征间的关联，丢失大量信息，降低了所得的最终图像特征的区分性。

      * 因此有学者提出netVLAD的方法。首先去掉CNN的最后一层，把它作为描述子，输出是 H×W×D 的向量，可以将其看作一个D维的描述子，总共有 H×W 个。相当于在整个图像上提取H×W 个D维描述子，然后对这H×W 个D维的描述子进行VLAD聚类，可以看做在CNN网络后面直接接一个netVLAD网络，如下图所示：

        ![netVLAD](G:\AI\DeepLearning-study\readme\netVLAD.png)

      * 还有学者提出基于对象的方法来解决以上问题。在生成图像区域时，使用基于内容的无监督对象生成方法，即通过图像颜色、纹理、边缘等视觉信息以聚类的方式来生成图像区域。常用方法有Selective search，如下图所示，Selective search在一张图片中提取1000－10000个bounding box，使之尽可能覆盖所有的物体，试验也证明，它的覆盖率能达到96%以上，足以提取丰富的局部区域特征。其特征描述也具有更高的区分性，同时基于对象特征进行融合，所得最终特征对场景中对象的空间布局变化也具有很好的鲁棒性。在聚合特征的过程时，采用上篇提到的VLAD算法，先将图像的局部区域特征进行聚类，然后统计一幅图像中所有区域特征与其相近聚类中心的累积残差来表示最终的图像特征。相对于最大池化算法，该方法考虑了区域特征间关联的同时对图像的局部信息有更细致的刻画，使得得到的最终图像特征对各类图像变换具有更高鲁棒性。 

    * Loss函数优化：

      * 在传统的分类网络中，一般是对大类如猫、狗、鸟等类别进行分类，但其对于个体级别的细粒度识别上略显不足，而tripletloss和contrastive loss等算法就更注重于高细粒度同类样本和不同样本之间的损失计算。 

      * triple Loss: 端到端、简单直接、自带聚类属性，特征高度嵌入

        ![triple_loss](G:\AI\DeepLearning-study\readme\triple_loss.png)

      * Contrastive Loss：![contrastive_loss](G:\AI\DeepLearning-study\readme\contrastive_loss.png)

    * 特征的哈希变换-CNNH![特征的哈希变换](G:\AI\DeepLearning-study\readme/特征的哈希变换.png)

       