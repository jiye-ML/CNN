## 序言


* 于是乎我们有了以下的经验总结：
    1. sigmoid在压缩数据幅度方面有优势，对于深度网络，使用sigmoid可以保证数据幅度不会有问题，这样数据幅度稳住了就不会出现太大的失误。
    2. sigmoid存在梯度消失的问题，在反向传播上有劣势，所以在优化的过程中存在不足
    3. relu不会对数据做幅度压缩，所以如果数据的幅度不断扩张，那么模型的层数越深，幅度的扩张也会越厉害，最终会影响模型的表现。
    4. 但是relu在反向传导方面可以很好地将“原汁原味”的梯度传到后面，这样在学习的过程中可以更好地发挥出来。
    
    
    

#### 有哪些

* sigmoid function [0, 1]: a = 1 / (1 + e ^ -z)
* tanh function [-1, 1]
* ReLU: 解决梯度消失问题，但是还是有问题，
* Leak ReLU

#### why 激活函数

* 非线性组合


## 正文


### 《Rectified nonlinearities improve neural network acstic models》
* [paper](paper/2013-Rectified%20nonlinearities%20improve%20neural%20network%20acstic%20models.pdf)
* why
    * 它的0值以外的偏导数是1。因此梯度消失不存在。 另外， ReLU在除了0以外的地方饱和，这潜在帮助了当隐层单元激活作为输入特征，分类的。
    * 如果我们相信稀疏激活对于一个隐藏单元的是重要的对于输入刺激的不变性，那么ReLU有很明显的优势。
    
 
### 《2015-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification》
* [paper](paper/2015-Delving%20Deep%20into%20Rectifiers%20Surpassing%20Human-Level%20Performance%20on%20ImageNet%20Classification.pdf)

    
    
### softmax

![](study_softmax/不同数字可能对应特征权重.png)
![](study_softmax/softmax_计算公式.png)

#### 为什么神经网络的最后一层都是softmax
* 因为softmax使用了sigmoid函数，再加上归一化，让预测出来的结果是一个概率，方便后面的计算。


## 后记


    
## 杂谈