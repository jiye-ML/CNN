## 序言

#### 有哪些

* sigmoid function [0, 1]: a = 1 / (1 + e ^ -z)
* tanh function [-1, 1]
* ReLU: 解决梯度消失问题，但是还是有问题，
* Leak ReLU

#### why 激活函数

* 非线性组合


## 正文


### 《Rectified nonlinearities improve neural network acstic models》
* [paper](paper/2013-Rectified%20nonlinearities%20improve%20neural%20network%20acstic%20models.pdf)


### softmax

![](study_softmax/不同数字可能对应特征权重.png)
![](study_softmax/softmax_计算公式.png)

#### 为什么神经网络的最后一层都是softmax
* 因为softmax使用了sigmoid函数，再加上归一化，让预测出来的结果是一个概率，方便后面的计算。


