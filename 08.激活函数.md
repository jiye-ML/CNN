## 序言

#### 有哪些

* sigmoid function [0, 1]: a = 1 / (1 + e ^ -z)
* tanh function [-1, 1]
* ReLU: 解决梯度消失问题，但是还是有问题，
* Leak ReLU

#### why 激活函数

* 非线性组合


## 正文


### 《Rectified nonlinearities improve neural network acstic models》
* [paper](paper/2013-Rectified%20nonlinearities%20improve%20neural%20network%20acstic%20models.pdf)
* why
    * 它的0值以外的偏导数是1。因此梯度消失不存在。 另外， ReLU在除了0以外的地方饱和，这潜在帮助了当隐层单元激活作为输入特征，分类的。
    * 如果我们相信稀疏激活对于一个隐藏单元的是重要的对于输入刺激的不变性，那么ReLU有很明显的优势。
    
 
### 《2015-Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification》
* [paper](paper/2015-Delving%20Deep%20into%20Rectifiers%20Surpassing%20Human-Level%20Performance%20on%20ImageNet%20Classification.pdf)

    
    
### softmax

![](study_softmax/不同数字可能对应特征权重.png)
![](study_softmax/softmax_计算公式.png)

#### 为什么神经网络的最后一层都是softmax
* 因为softmax使用了sigmoid函数，再加上归一化，让预测出来的结果是一个概率，方便后面的计算。


## 后记


    
## 杂谈