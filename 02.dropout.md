## 把dropout 专门写出来



* dropout的目标是在指数级数量的神经玩过上近似这个过程；
* dropout训练与bagging训练不太一样，在bagging的情况下，所有模型是独立的，dropout情况下，模型是共享参数的，
  其中每个模型继承的父神经网络参数的不同子集。
* 参数共享使得在有限可用的内存下代表指数数量的模型变得可能。
  ![](readme/dropout_01.png)

* bagging集成必须从所有成员的累积投票做一个预测。我们将这个过程称为推断。
* dropout的方式不能依赖于任何的特征，因为任何特征都有可能被丢弃，
* dropOut是用来防止过拟合，所以除非你的网络过拟合， 否则不要使用。

#### 下面操作与Dropout的类似操作

* [集成学习](https://github.com/jiye-ML/ML-study/blob/master/08.ensemble_learning.md)



###  dropout

* [paper](paper/2014%20-%20Dropout%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from.pdf)

> 这篇论文简直就是dropout的最佳使用手册；



### 架构形式

![1540261833753](readme/dropout_结构_01.png)

* 训练时候和测试时候每个单元使用情况：

  ![1540262477791](readme/dropout_每个单元使用方法_01.png)

* 带有dropout的网络：

  ![1540277134950](readme/dropout_架构_02.png)

* 公式表达：

![1540277173672](readme/dropout_架构_公式.png)

### 反向传播

* 没有用到的参数反向传播梯度为0；


## 《2013-Regularization of Neural Networks using DropConnect》
* [paper](paper/2013-Regularization%20of%20Neural%20Networks%20using%20DropConnect.pdf)

### 动机

* 改进dropout

### 结构

* Dropout不同的是，它不是随机将隐含层节点的输出清0,而是将节点中的每个与其相连的输入权值以1-p的概率清0。

  ![1540279512746](readme/dropconnet_01.png)

* 对于上一层来说输出是全部的，但是对于本层来说，每一层接受到的输入会随着掩码矩阵而改变。

  ![1540281245012](readme/dropconnet_结构_01.png)

## 《2013-Maxout Networks》
* [paper](paper/2013-Maxout%20Networks.pdf)

* [深度学习（二十三）Maxout网络学习](https://blog.csdn.net/hjimce/article/details/50414467)

### 结构

* 函数

  ![1540287155439](readme/maxout_函数_01.png)