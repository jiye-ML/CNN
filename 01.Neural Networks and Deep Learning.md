## 神经网络

* [神经网络训练时对输入有什么特别的要求吗？](https://www.zhihu.com/question/47908908)


### 神经元

* 是一个函数功能的单元，获得输入，然后经过函数处理，得到一个输出。
![](01.Neural%20Networks%20and%20Deep%20Learning/神经元表示.png)

* 感知机三要素
    * 直接组合输入计算输出
    * 输入可以加入权重
    * 加入偏置值

### 什么是NN

* 一系列神经元组成的集合
![](01.Neural%20Networks%20and%20Deep%20Learning/神经网络表示.png)
* 这里其实采用的是全连接方式，画出来的是表示权值大，没画出来的表示权值足够小，可以忽略


### 有监督学习

![](01.Neural%20Networks%20and%20Deep%20Learning/有监督学习概况.png)


### nn例子
![](01.Neural%20Networks%20and%20Deep%20Learning/nn例子.png)


### 为什么深度学习变得好使

* 数据
* 计算能力
* 算法
![](01.Neural%20Networks%20and%20Deep%20Learning/为什么nn变得好使了.png)


### 探索的过程
* idea
* code
* 实验

### hiton

* 读一些论文，但不要读太多，
* 注意到一些您想每个人都做错的事， 你发现这样做不对，你想办法做对，当有人告诉你这不好, 要用原来的，
 而我有一个很好的原则来帮助人们保有它，要么是你的直觉很好，要么不是。
如果你的直觉是好的你应该跟随它们，你最终会成功，如果你的直觉不是好的，你做什么都无所谓。
复现已有的论文，你会发现所有必要的小技巧来使他可行。
* 永远不要停止程序设计。




## 逻辑回归作为神经网络

> 逻辑回归本质是一个单隐层神经网络，因为只需要一次激活。

### 计算图

* 前向传播
* 本层的输入和前一层输出有关
![](01.Neural%20Networks%20and%20Deep%20Learning/Foward.png)

* 反向传播
* 本层的输入和后一个层的输出有关
![](01.Neural%20Networks%20and%20Deep%20Learning/backward.png)


### peter abbeel 

* 你要看到工作与现实影响之间的联系


## 梯度下降

* 各个激活函数的求导



## 随机初始化

> 这里说的是参数

* 初始化为0是不行的。
* 导致相同参数下的神经元功能相同
* 使用比较小的随机值， 因为激活函数的原因，太大的初始值，导数很小，sigmoid


* [纯手工单隐层神经网络实现](01.Neural%20Networks%20and%20Deep%20Learning/classification+with+one+hidden+layer+纯手工.ipynb)


## lan Goodfellow

* 看花书，
* 找一个感兴趣的东西，做出来
* 上传到GitHub上



## 多层神经网络

* 当我们在构建自己的神经网络时，如果试图尽量减少发生问题的概率，就必须要很系统很仔细地，
处理相关矩阵的维度 当我自己在写代码的时候 通常会拿一张纸 仔细地考虑我正在使用的矩阵的维度
![](01.Neural%20Networks%20and%20Deep%20Learning/w的维度.png)

 
#### 层数 & 每一层神经元个数 影响

* 层数：
    * 输入的数据中有多少特征
    * 前面层学习简单的特征，后面层组合学习到更复杂的特征
* 每一层神经元个数
    * 输入中有多少种可能特征 组合方式
    
![](01.Neural%20Networks%20and%20Deep%20Learning/层数和每层神经元个数.png)
 
 
* [DNN 纯手工搭建](01.Neural%20Networks%20and%20Deep%20Learning/DNN_纯手工.ipynb)
* [自己搭建DNN的使用](01.Neural%20Networks%20and%20Deep%20Learning/DNN_Application.ipynb)




* [A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)
    * 神经网络可以拟合任何函数；即使是一个隐藏层；
    * 警告：
        1. 拟合的是任何函数的一个近似
        2. 只能近似拟合连续函数；使用连续函数拟合不连续函数效果很好，所以也可以使用神经网络学习；
    * 用sigmoid模拟渐跃函数，step=-(b/w)；
    * 利用渐跃函数逼近原来函数的过程等价于积分过程。

