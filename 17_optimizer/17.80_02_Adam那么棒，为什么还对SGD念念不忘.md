- [Adam那么棒，为什么还对SGD念念不忘 (1) —— 一个框架看懂优化算法](https://zhuanlan.zhihu.com/p/32230623)
- [Adam那么棒，为什么还对SGD念念不忘 (3)—— 优化算法的选择与使用策略](https://zhuanlan.zhihu.com/p/32338983)
  - [自己总结请看](14.一个框架看懂优化算法.docx)
- [Adam那么棒，为什么还对SGD念念不忘 (2)—— Adam的两宗罪](https://zhuanlan.zhihu.com/p/32262540)
  - 大家都是殊途同归，只是相当于在SGD基础上增加了各类学习率的主动控制。如果不想做精细的调优，那么Adam显然最便于直接拿来上手。
  - Adam罪状一：可能不收敛
    - SGD学习率恒定
    - AdaGrad二阶动量不断积累，单调递增，学习率单调递减，最后收敛到0，模型也得以收敛
    - AdaDelta和Adam则不然，二阶动量固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，
      使得二阶动量可能时大时小，不是单调变化，这可能在训练后期引起学习率震荡，导致模型无法收敛；
  - Adam罪状二：可能错过全局最优解
    - Adam的收敛速度比SGD要快，但最终收敛的结果并没有SGD好。他们进一步实验发现，主要是后期Adam的学习率太低，影响了有效的收敛。
  - 到底该用Adam还是SGD？三点小提醒
    1. 前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。
    2. 但这提醒了我们，理解数据对于设计算法的必要性。优化算法的演变历史，都是基于对数据的某种假设而进行的优化，
       那么某种算法是否有效，就要看你的数据是否符合该算法的胃口了。