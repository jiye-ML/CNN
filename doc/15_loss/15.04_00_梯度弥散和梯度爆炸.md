### 原因分析

- 通常神经网路用的激活函数是sigmoid，而sigmoid函数能将负无穷大到正无穷大的数据映射到0-1之间，并且这个函数的导数
  f'(x)=f(x)*(1-f(x))。因此两个0-1之间的数相乘，得到的结果就会变得很小，反向传播是逐层对函数偏导数相乘。因此，
  当网络层数非常深的时候，最后一层产生的偏差就因为成了很多小于1的数二越来越小，最终为0，从而导致，浅层的权重没有更新。
- 梯度爆炸是因为初始化权重过大，前面层会比后面层变化的更快。
- [梯度弥散与梯度爆炸](https://www.cnblogs.com/yangmang/p/7477802.html)
- [详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526)
  - 对于本层来说：如果 `z=f(wa2 + b2)`，则导数为 `dz = (df/dw)*w*da2`,
    - 如果 `(df/dw)*w`很大就会使得`dz`很大，
    - 如果`(df/dw)*w`很小就会使得`dz`很小，出现不学习的情况，更新很慢，需要更大的学习率。
  - 对于梯度反向回传来说：过小的`dz`，会使得梯度消失，过大的`dz`会使得梯度爆炸；这里`(df/dw)`可以理解为激活函数的
    导数，如果使用sigmod，那么双端饱和，梯度很小，dz自然会很小，层数加深，就会消失。
- [bp算法中为什么会产生梯度消失？](https://www.zhihu.com/question/49812013)
- [Why are deep neural networks hard to train?](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem)



