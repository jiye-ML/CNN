#### ReLU激活函数

- 用ReLU替代sigmoid函数，ReLU>0的区域不会饱和；ReLU有一定的稀疏性，主要解决梯度消失问题。
  - 优点： 使用 ReLU得到的SGD的收敛速度会比 sigmoid/tanh 快。这是因为它是linear，而且ReLU只需要一个阈值就可以得到激活值，
    不用去计算复杂的运算。
  - 缺点： 训练过程该函数不适应较大梯度输入，因为在参数更新以后，ReLU的神经元不会再有激活的功能，导致梯度永远都是零。
  - 为了针对以上的缺点，又出现Leaky-ReLU、P-ReLU、R-ReLU三种拓展激活函数。
- 个人理解：
  - 主要是计算方便，计算快，带了的一半的神经元失活问题，应该是网络本身的冗余，之后有些论文就像神经网络压缩了，
    比如VGG压缩到了很小，