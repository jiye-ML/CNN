https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b



* [PyTorch 中使用优化器 Rectified Adam (RAdam)]( https://www.pytorchtutorial.com/pytorch-use-rectified-adam-radam/ )

* [中国博士生提出最先进AI训练优化器，收敛快精度高，网友亲测：Adam可以退休了]( https://mp.weixin.qq.com/s/scGkuMJ4lZULhmK69vWYpA )

* [如何看待最新提出的Rectified Adam (RAdam)？]( https://www.zhihu.com/question/340834465 )

*  https://github.com/LiyuanLucasLiu/RAdam 


* [paper](paper/30.43.10-19-On-the-Variance-of-the-Adaptive-Learning-Rate-and-Beyond.pdf)

## when

* CVPR 2019

## what

* 找到一种快速稳定的优化算法，是所有AI研究人员的目标。

  但是鱼和熊掌不可兼得。Adam、RMSProp这些算法虽然收敛速度很快，当往往会掉入局部最优解的“陷阱”；原始的SGD方法虽然能收敛到更好的结果，但是训练速度太慢。

  最近，一位来自UIUC的中国博士生Liyuan Liu提出了一个新的优化器**RAdam**。

  它兼有Adam和SGD两者的优点，既能保证收敛速度快，也不容易掉入局部最优解，而且收敛结果对学习率的初始值非常不敏感。在较大学习率的情况下，RAdam效果甚至还优于SGD。

###  who （动机）

- RAdam意思是“整流版的Adam”（Rectified Adam），它能根据方差分散度，动态地打开或者关闭自适应学习率，并且提供了一种不需要可调参数学习率预热的方法。

  一位Medium网友Less Wright在测试完RAdam算法后，给予了很高的评价：

  > RAdam可以说是最先进的AI优化器，可以永远取代原来的Adam算法了。

  目前论文作者已将RAdam开源，FastAI现在已经集成了RAdam，只需几行代码即可直接调用。

  ![img](readme\30.43-10-RAdam效果图.png)

## where

### 创新点

* 想造出更强的优化器，就要知道前辈们的问题出在哪：

  像Adam这样的优化器，的确可以快速收敛，也因此得到了广泛的应用。

  但有个重大的缺点是不够鲁棒，常常会收敛到不太好的局部最优解 (Local Optima) ，这就要靠**预热** (Warmup)来解决——

  最初几次迭代，都用很小的学习率，以此来缓解收敛问题。

  为了证明预热存在的道理，团队在IWSLT’14德英数据集上，测试了**原始Adam**和**带预热的Adam**。

  结果发现，一把预热拿掉，Transformer语言模型的训练复杂度 (Perplexity) ，就从10增到了500。

  另外，BERT预训练也是差不多的情况。

  为什么预热、不预热差距这样大？团队又设计了两个变种来分析：





## how

### 缺乏样本，是问题根源

一个变种是**Adam-2k**：

在前2000次迭代里，只有自适应学习率是一直更新的，而动量 (Momentum) 和参数都是固定的。除此之外，都沿袭了原始Adam算法。

实验表明，在给它2000个额外的样本来估计自适应学习率之后，收敛问题就消失了：

![img](readme\30.43-10-Adam-2k-效果.png)

 另外，足够多的样本可以避免梯度分布变扭曲 (Distorted) ： 

![img](E:\jiye-学习\AI\DL_Loss_Optimize_Metric\readme\30.43-10-Adam-梯度图-01.png)

![img](E:\jiye-学习\AI\DL_Loss_Optimize_Metric\readme\30.43-10-Adam-梯度图-02.png)

这些发现证明了一点：**早期缺乏足够数据样本，就是收敛问题的根源**。

下面就要证明，可以通过降低自适应学习率的方差来弥补这个缺陷。

### 降低方差，可解决问题

一个直接的办法就是：

![img](E:\jiye-学习\AI\DL_Loss_Optimize_Metric\readme\30.43-10-降低方差方法-01.png)

把ψ-cap里面的ϵ增加。假设ψ-cap(. ) 是均匀分布，方差就是1/12ϵ^2。

这样就有了另一个变种**Adam-eps**。开始把ϵ设成一个可以忽略的1×10^-8，慢慢增加，到不可忽略的1×10^-4。

从实验结果看，它已经没有Adam原本的收敛问题了：



## how much

### 实验结果



## why （为什么好）



